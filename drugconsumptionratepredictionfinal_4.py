# -*- coding: utf-8 -*-
"""DrugConsumptionRatePredictionFinal-4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XwWdqVtizoM7OQC7Ae423KJKLXfr04OS

STEP-01: DATA COLLECTION
"""

# import all necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# load the dataset
from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/Drug Consumption dataset/Drug_Consumption.csv')
df.head(n=4)

print(df.dtypes)

df.shape

# check for missing values ---> handling missing values
df.isnull().sum()

print(df.columns)

df['Age'].value_counts()

df['Country'].value_counts()

df['Ethnicity'].value_counts()

df['Education'].value_counts()

df['Gender'].value_counts()

plt.style.use('dark_background')

plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='Age', order=['18-24', '25-34', '35-44', '45-54', '55-64', '65+'], palette=['skyblue', 'salmon','orange','green','red','violet'])

plt.title('Distribution of Age Groups')
plt.xlabel('Age Group')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(6, 4))
sns.countplot(data=df, x='Gender', palette=['skyblue', 'salmon'])
plt.title('Gender Distribution')
plt.xlabel('Gender')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(data=df, y='Ethnicity', order=df['Ethnicity'].value_counts().index,palette=['skyblue', 'salmon','orange','green','red','violet'])
plt.title('Ethnicity Distribution')
plt.xlabel('Count')
plt.ylabel('Ethnicity')
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(data=df, y='Country', order=df['Country'].value_counts().index,palette=['skyblue', 'salmon','orange','green','red','violet','white'])
plt.title('Country Distribution')
plt.xlabel('Count')
plt.ylabel('Country')
plt.show()

plt.figure(figsize=(12, 6))
sns.countplot(data=df, y='Education', order=df['Education'].value_counts().index,palette=['skyblue', 'salmon','orange','green','red','violet','white','pink','yellow'])
plt.title('Education Level Distribution')
plt.xlabel('Count')
plt.ylabel('Education Level')
plt.show()

plt.figure(figsize=(10, 8))
corr = df[['Nscore', 'Escore', 'Oscore', 'AScore', 'Cscore', 'Impulsive', 'SS']].corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Heatmap of Personality Traits')
plt.show()

"""STEP-02: DATA PRE-PROCESSING"""

# drop unwanted columns like - ID which is not important for processing
df.drop(['ID'], axis=1, inplace=True)

df.head(n=4)

df.columns

# Convert Age to numerical values
age_mapping = {'18-24': 0, '25-34': 1, '35-44': 2, '45-54': 3, '55-64': 4, '65+': 5}
df['Age'] = df['Age'].map(age_mapping)

# Convert drug use categories to binary: (User=1, Non-user=0)
# Ensure we apply this to only drug-related columns
traits = ['Nscore', 'Escore', 'Oscore', 'AScore', 'Cscore', 'Impulsive', 'SS']
categorical_cols = ['Gender', 'Education', 'Country', 'Ethnicity']
non_drug_cols = ['Age'] + traits + list(df.columns[df.columns.str.startswith(tuple(categorical_cols))])
drug_cols = [col for col in df.columns if col not in non_drug_cols]

df[drug_cols] = df[drug_cols].applymap(lambda x: 1 if x in ['CL6', 'CL5', 'CL4'] else 0)

df.head(n=4)

df.columns

"""STEP-03: FEATURE SELECTION AND STANDARDIZATION"""

# Step 1: Binarize drug columns â€” 1 = user, 0 = non-user
drug_cols_filtered = [col for col in drug_cols if df[col].nunique() > 1 and df[col].isnull().sum() < len(df) * 0.9]
drug_user_counts = df[drug_cols_filtered].sum().sort_values(ascending=False)
# Step 3: Display most consumed drug
most_consumed_drug = drug_user_counts.idxmax()
print(f"Most Consumed Drug: {most_consumed_drug} with {drug_user_counts.max()} users")

# Step 4: Plot all drug usage counts
plt.figure(figsize=(12, 6))
sns.barplot(x=drug_user_counts.values, y=drug_user_counts.index, palette='crest')
plt.title("Most Consumed Drugs (Based on Number of Users)")
plt.xlabel("Number of Users")
plt.ylabel("Drug Name")
plt.grid(True, axis='x', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# Standardize features
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

# Select only numerical features for scaling
numerical_features = df.drop(columns=drug_cols).select_dtypes(include=np.number).columns
X_scaled = scaler.fit_transform(df[numerical_features])

"""PRINCIPAL COMPONENT ANALYSIS - DATA REDUCTION TECHNIQUE"""

# Step 2: PCA Analysis (Retain 95% Variance)
from sklearn.decomposition import PCA
pca = PCA(n_components=0.95)
principal_components = pca.fit_transform(X_scaled)


plt.figure(figsize=(8, 5))
plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Explained Variance by PCA Components')
plt.grid()
plt.show()


pca_df = pd.DataFrame(principal_components, columns=[f'PC{i+1}' for i in range(principal_components.shape[1])])


print("PCA-Reduced Data (First 5 Rows):")
print(pca_df.head())

df.head(n=4)

df.columns

"""ANALYZE TRENDS THROUGH VISUVALISATIONS BASED ON CURRENT DATA"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# 1. Calculate user and non-user counts for each drug
user_counts = df[drug_cols].sum()
non_user_counts = len(df) - user_counts

# 2. Create a DataFrame for plotting
drug_usage_df = pd.DataFrame({'Drug': drug_cols,
                               'Users': user_counts,
                               'Non-Users': non_user_counts})

# 3. Melt the DataFrame for easier plotting with seaborn
drug_usage_df = pd.melt(drug_usage_df, id_vars=['Drug'], var_name='Category', value_name='Count')

# 4. Create the bar plot
plt.figure(figsize=(12, 6))  # Adjust figure size as needed
sns.barplot(x='Drug', y='Count', hue='Category', data=drug_usage_df, palette='viridis')
plt.title('Distribution of Drug Users vs. Non-Users')
plt.xlabel('Drug')
plt.ylabel('Number of Individuals')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd


# 1. Group by age and calculate average drug usage
age_drug_usage = df.groupby('Age')[drug_cols].mean().reset_index()

# 2. Melt the DataFrame for easier plotting
age_drug_usage_melt = pd.melt(age_drug_usage, id_vars=['Age'], var_name='Drug', value_name='Usage')


# 3. Create the line plot (with improvements)
plt.figure(figsize=(12, 8))  # Increase figure size for better readability
sns.lineplot(x='Age', y='Usage', hue='Drug', data=age_drug_usage_melt,
             palette='viridis', marker='o', markersize=8)  # Add markers

plt.title('Age Group vs. Drug Consumption', fontsize=16)  # Increase title font size
plt.xlabel('Age Group', fontsize=12)
plt.ylabel('Average Drug Usage', fontsize=12)
plt.xticks(age_drug_usage['Age'], ['18-24', '25-34', '35-44', '45-54', '55-64', '65+'], fontsize=10)
plt.yticks(fontsize=10)

plt.grid(True, linestyle='--', alpha=0.7)  # Add gridlines
plt.legend(title='Drug', loc='upper left', bbox_to_anchor=(1, 1))  # Adjust legend position
plt.ylim(0, 1) # Set y-axis limit between 0 and 1.

plt.tight_layout()
plt.show()

df.columns

# Bar Chart: Drug Usage by Gender
gender_drug_usage = df.groupby("Gender")[drug_cols].mean().T
gender_drug_usage.plot(kind="bar", stacked=True, figsize=(12, 6), colormap="viridis")
plt.title("Drug Usage by Gender (Stacked Bar Chart)")
plt.xlabel("Drugs")
plt.ylabel("Proportion of Users")
plt.legend(title="Gender")
plt.xticks(rotation=45)
plt.yticks(rotation=45)
plt.show()

# Bar Chart: Drug Usage by Country
country_drug_usage = df.groupby("Country")[drug_cols].mean().T
country_drug_usage.plot(kind="bar", stacked=True, figsize=(12, 6), colormap="coolwarm")
plt.title("Drug Usage by Country (Stacked Bar Chart)")
plt.xlabel("Drugs")
plt.ylabel("Proportion of Users")
plt.legend(title="Country")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Pie Chart: Drug Usage by Education
import matplotlib.pyplot as plt
import pandas as pd
education_drug_usage = df.groupby("Education")[drug_cols].sum()

# Sum total drug usage across all drugs for each education level
total_usage_by_education = education_drug_usage.sum(axis=1)

plt.figure(figsize=(10, 10))
plt.pie(total_usage_by_education, labels=total_usage_by_education.index,
        autopct='%1.1f%%', startangle=90, colors=plt.cm.Paired.colors)
plt.title("Drug Usage by Education Level")
plt.show()

# Line Plot: Drug counts by Ethinicity
ethnicity_drug_usage = df.groupby("Ethnicity")[drug_cols].sum()
ethnicity_drug_usage.plot(kind="line", figsize=(12, 6), colormap="viridis")
plt.title("Drug Counts by Ethnicity")
plt.xlabel("Drugs")
plt.ylabel("Count")
plt.legend(title="Ethnicity", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.xticks(rotation=45)
plt.yticks(rotation=45)
plt.show()

import seaborn as sns

# Calculate correlation matrix (or another co-occurrence metric)
drug_correlation = df[drug_cols].corr()

# Create heatmap
plt.figure(figsize=(15,10))
sns.heatmap(drug_correlation, annot=True, cmap='viridis')
plt.title('Drug Co-occurrence Heatmap')
plt.show()

"""STEP-04: MODEL TRAINING & EVALUATION"""

df.head(n=4)

columns_to_drop = ['OverallEducation', 'DrugUser', 'OverallCountry', 'TotalDrugUsage', 'TotalDrugsUsed', 'Country']
df = df.drop(columns=columns_to_drop, errors='ignore')

df.columns

"""CASE-01

TRAIN-TEST SPLIT -- 80 percent training data and 20 percent testing data

Helps identify which demographic factors (age, gender, education, etc.) have a significant impact on all drug consumption.

Provides probability scores, which are useful for ranking individuals based on their likelihood of drug use.
"""

!pip install catboost

"""1.   MODEL-01: LOGISTIC REGRESSION
2.   MODEL-02: SUPPORT VECTOR MACHINE (SVC)
3.   MODEL-03: RANDOM FOREST (RFC)
4.   MODEL-04: DECISION TREE (DT)
5.   MODEL-05: NAIVE-BAYES (NB)
6.   MODEL-06: KNN CLASSIFIER (KNN)
7.   MODEL-07: XG-BOOST CLASSIFIER (XGB)
8.   MODEL-08: MLP CLASSIFIER (MLP)
9.   MODEL-09: BAYSIEN CLASSIFIER (NB)
10.  MODEL-10: SGD CLASSIFIER (SGD)
11.  MODEL-11: ADABOOST CLASSIFIER (ADABOOST)
12. MODEL-12: RIDGE CLASSIFIER (L1)
14. MODEL-13: LIGHTGBM CLASSIFIER (LGBM)
15. MODEL-14: CATABOOST CLASSIFIER (CATABOOST)



"""

df.columns

import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from catboost import CatBoostClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.metrics import classification_report
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier



categorical_features = ['Gender', 'Education', 'Ethnicity']
numerical_features = ['Age', 'Nscore', 'Escore', 'Oscore', 'AScore', 'Cscore', 'Impulsive', 'SS']


encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
encoded_features = encoder.fit_transform(df[categorical_features])
encoded_feature_names = encoder.get_feature_names_out(categorical_features)
encoded_df = pd.DataFrame(encoded_features, columns=encoded_feature_names, index=df.index)


X = pd.concat([df[numerical_features], encoded_df], axis=1)


scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


results = {}

drug_labels = ['Alcohol', 'Amphet','Caff','Amyl','Benzos','Cannabis','Choc','Crack', 'Ecstasy', 'Heroin', 'Ketamine', 'Legalh', 'LSD', 'Meth', 'Mushrooms', 'Nicotine', 'Semer', 'VSA']


for drug in drug_labels:
    print(f"Training models for {drug} with Cross-Validation...")


    le = LabelEncoder()
    y = le.fit_transform(df[drug])


    models = {
        'Logistic Regression': LogisticRegression(class_weight='balanced', max_iter=1000),
        'Support Vector Machine': SVC(class_weight='balanced'),
        'Random Forest': RandomForestClassifier(class_weight='balanced'),
        'Decision Tree': DecisionTreeClassifier(class_weight='balanced'),
        'Naive Bayes': GaussianNB(),
        'K-Nearest Neighbors': KNeighborsClassifier(),
        'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),
        'MLP': MLPClassifier(max_iter=500),
        'CatBoost': CatBoostClassifier(verbose=0),
        'Ridge Classifier': RidgeClassifier(class_weight='balanced'),
        'SGD Classifier': SGDClassifier(class_weight='balanced', max_iter=1000),
        'AdaBoost Classifier': AdaBoostClassifier(),
        'LightGBM': LGBMClassifier()
    }

    drug_results = {}


    for name, model in models.items():
        scores = cross_val_score(model, X_scaled, y, cv=5, scoring='accuracy')
        acc = scores.mean()
        model.fit(X_scaled, y)
        report = classification_report(y, model.predict(X_scaled))
        drug_results[name] = {
            'Cross-Validated Accuracy': acc,
            'Classification Report': report
        }

    results[drug] = drug_results


for drug, model_results in results.items():
    print(f"\nResults for {drug}:")
    for model_name, metrics in model_results.items():
        print(f"\n{model_name}:")
        print(f"Cross-Validated Accuracy: {metrics['Cross-Validated Accuracy']:.4f}")
        print(f"Classification Report:\n{metrics['Classification Report']}")

import matplotlib.pyplot as plt
import seaborn as sns

plt.style.use('dark_background')

# For each drug, plot model accuracy comparison
for drug, model_results in results.items():
    plt.figure(figsize=(12, 6))
    model_names = list(model_results.keys())
    accuracies = [model_results[model]['Cross-Validated Accuracy'] for model in model_names]


    plot_df = pd.DataFrame({
        'Model': model_names,
        'Accuracy': accuracies
    }).sort_values(by='Accuracy', ascending=False)

    # Bar plot
    sns.barplot(data=plot_df, x='Accuracy', y='Model', palette='viridis')

    plt.title(f'Model Comparison for {drug}')
    plt.xlabel('Cross-Validated Accuracy')
    plt.ylabel('Model')
    plt.xlim(0, 1)
    plt.tight_layout()
    plt.show()

# Find the model with highest accuracy for the most consumed drug
best_model_info = max(results[most_consumed_drug].items(), key=lambda x: x[1]['Cross-Validated Accuracy'])

best_model_name = best_model_info[0]
best_model_accuracy = best_model_info[1]['Cross-Validated Accuracy']

print(f"\nBest Model for Predicting '{most_consumed_drug}': {best_model_name} with Accuracy = {best_model_accuracy:.4f}")

"""STEP-05: MODEL OPTIMIZATION"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report


# Encode the target label
le = LabelEncoder()
y = le.fit_transform(df[most_consumed_drug])

rf = RandomForestClassifier(class_weight='balanced', random_state=42)


param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'max_features': ['sqrt']
}


grid_search = GridSearchCV(
    estimator=rf,
    param_grid=param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

# Fit on the training data
grid_search.fit(X_scaled, y)
best_rf = grid_search.best_estimator_
best_accuracy = grid_search.best_score_

# Print results
print(f"\n New Cross-Validated Accuracy after Hyper-Tuning: {best_accuracy:.4f}")
print(f" Best Hyperparameters: {grid_search.best_params_}")

y_pred = best_rf.predict(X_scaled)
print(classification_report(y, y_pred))

"""CASE-02

TRAIN-TEST SPLIT -- 70 percent training data and 30 percent testing data
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from catboost import CatBoostClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

# One-hot encode categorical features
categorical_features = ['Gender', 'Education', 'Ethnicity']
numerical_features = ['Age', 'Nscore', 'Escore', 'Oscore', 'AScore', 'Cscore', 'Impulsive', 'SS']

encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
encoded_features = encoder.fit_transform(df[categorical_features])
encoded_feature_names = encoder.get_feature_names_out(categorical_features)
encoded_df = pd.DataFrame(encoded_features, columns=encoded_feature_names, index=df.index)

X = pd.concat([df[numerical_features], encoded_df], axis=1)


scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


drug_labels = ['Alcohol', 'Amphet', 'Caff', 'Amyl', 'Benzos', 'Cannabis', 'Choc', 'Crack',
               'Ecstasy', 'Heroin', 'Ketamine', 'Legalh', 'LSD', 'Meth', 'Mushrooms',
               'Nicotine', 'Semer', 'VSA']


models = {
    'Logistic Regression': LogisticRegression(class_weight='balanced', max_iter=1000),
    'Support Vector Machine': SVC(class_weight='balanced'),
    'Random Forest': RandomForestClassifier(class_weight='balanced'),
    'Decision Tree': DecisionTreeClassifier(class_weight='balanced'),
    'Naive Bayes': GaussianNB(),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),
    'MLP': MLPClassifier(max_iter=500),
    'CatBoost': CatBoostClassifier(verbose=0),
    'Ridge Classifier': RidgeClassifier(class_weight='balanced'),
    'SGD Classifier': SGDClassifier(class_weight='balanced', max_iter=1000),
    'AdaBoost Classifier': AdaBoostClassifier(),
    'LightGBM': LGBMClassifier()
}

results = {}


for drug in drug_labels:
    # Encode target
    le = LabelEncoder()
    y = le.fit_transform(df[drug])

    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

    drug_results = {}

    for name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        acc = accuracy_score(y_test, y_pred)
        report = classification_report(y_test, y_pred)
        drug_results[name] = {
            'Test Accuracy': acc,
            'Classification Report': report
        }

    results[drug] = drug_results

for drug, model_results in results.items():
    print(f"\n Results for {drug}:")
    for model_name, metrics in model_results.items():
        print(f"\n {model_name}")
        print(f"Test Accuracy: {metrics['Test Accuracy']:.4f}")
        print(f"Classification Report:\n{metrics['Classification Report']}")

import matplotlib.pyplot as plt
import seaborn as sns
plot_data = []

for drug, model_results in results.items():
    for model_name, metrics in model_results.items():
        plot_data.append({
            'Drug': drug,
            'Model': model_name,
            'Test Accuracy': metrics['Test Accuracy']
        })


accuracy_df = pd.DataFrame(plot_data)
for drug in drug_labels:
    plt.figure(figsize=(12, 5))
    subset = accuracy_df[accuracy_df['Drug'] == drug]
    sns.barplot(data=subset, x='Model', y='Test Accuracy', palette='viridis')
    plt.title(f'Model Comparison for {drug}')
    plt.xticks(rotation=45, ha='right')
    plt.ylim(0, 1)
    plt.tight_layout()
    plt.show()

# Find the model with highest accuracy for the most consumed drug
best_model_info = max(results[most_consumed_drug].items(), key=lambda x: x[1]['Test Accuracy'])

best_model_name = best_model_info[0]
best_model_accuracy = best_model_info[1]['Test Accuracy']

print(f"\nBest Model for Predicting '{most_consumed_drug}': {best_model_name} with Accuracy = {best_model_accuracy:.4f}")

"""HYPER-PARAMETER TUNING"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import AdaBoostClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report


# Encode the target label
le = LabelEncoder()
y = le.fit_transform(df[most_consumed_drug])

ad = AdaBoostClassifier()


param_grid = {
    'n_estimators': [50, 100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.5, 1.0],
    'algorithm': ['SAMME', 'SAMME.R']
}


grid_search = GridSearchCV(
   estimator=ad,
    param_grid=param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

# Fit on the training data
grid_search.fit(X_scaled, y)
best_ad = grid_search.best_estimator_
best_accuracy = grid_search.best_score_

# Print results
print(f"\n New Cross-Validated Accuracy after Hyper-Tuning: {best_accuracy:.4f}")
print(f" Best Hyperparameters: {grid_search.best_params_}")

y_pred = best_ad.predict(X_scaled)
print(classification_report(y, y_pred))

"""
CASE-03

TRAIN-TEST SPLIT -- 90 percent training data and `10 percent testing data"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from catboost import CatBoostClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

# One-hot encode categorical features
categorical_features = ['Gender', 'Education', 'Ethnicity']
numerical_features = ['Age', 'Nscore', 'Escore', 'Oscore', 'AScore', 'Cscore', 'Impulsive', 'SS']

encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
encoded_features = encoder.fit_transform(df[categorical_features])
encoded_feature_names = encoder.get_feature_names_out(categorical_features)
encoded_df = pd.DataFrame(encoded_features, columns=encoded_feature_names, index=df.index)

X = pd.concat([df[numerical_features], encoded_df], axis=1)


scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


drug_labels = ['Alcohol', 'Amphet', 'Caff', 'Amyl', 'Benzos', 'Cannabis', 'Choc', 'Crack',
               'Ecstasy', 'Heroin', 'Ketamine', 'Legalh', 'LSD', 'Meth', 'Mushrooms',
               'Nicotine', 'Semer', 'VSA']


models = {
    'Logistic Regression': LogisticRegression(class_weight='balanced', max_iter=1000),
    'Support Vector Machine': SVC(class_weight='balanced'),
    'Random Forest': RandomForestClassifier(class_weight='balanced'),
    'Decision Tree': DecisionTreeClassifier(class_weight='balanced'),
    'Naive Bayes': GaussianNB(),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),
    'MLP': MLPClassifier(max_iter=500),
    'CatBoost': CatBoostClassifier(verbose=0),
    'Ridge Classifier': RidgeClassifier(class_weight='balanced'),
    'SGD Classifier': SGDClassifier(class_weight='balanced', max_iter=1000),
    'AdaBoost Classifier': AdaBoostClassifier(),
    'LightGBM': LGBMClassifier()
}

results = {}


for drug in drug_labels:
    # Encode target
    le = LabelEncoder()
    y = le.fit_transform(df[drug])

    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.1, random_state=42)

    drug_results = {}

    for name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        acc = accuracy_score(y_test, y_pred)
        report = classification_report(y_test, y_pred)
        drug_results[name] = {
            'Test Accuracy': acc,
            'Classification Report': report
        }

    results[drug] = drug_results

for drug, model_results in results.items():
    print(f"\n Results for {drug}:")
    for model_name, metrics in model_results.items():
        print(f"\n {model_name}")
        print(f"Test Accuracy: {metrics['Test Accuracy']:.4f}")
        print(f"Classification Report:\n{metrics['Classification Report']}")

import matplotlib.pyplot as plt
import seaborn as sns
plot_data = []

for drug, model_results in results.items():
    for model_name, metrics in model_results.items():
        plot_data.append({
            'Drug': drug,
            'Model': model_name,
            'Test Accuracy': metrics['Test Accuracy']
        })


accuracy_df = pd.DataFrame(plot_data)
for drug in drug_labels:
    plt.figure(figsize=(12, 5))
    subset = accuracy_df[accuracy_df['Drug'] == drug]
    sns.barplot(data=subset, x='Model', y='Test Accuracy', palette='viridis')
    plt.title(f'Model Comparison for {drug}')
    plt.xticks(rotation=45, ha='right')
    plt.ylim(0, 1)
    plt.tight_layout()
    plt.show()

# Find the model with highest accuracy for the most consumed drug
best_model_info = max(results[most_consumed_drug].items(), key=lambda x: x[1]['Test Accuracy'])

best_model_name = best_model_info[0]
best_model_accuracy = best_model_info[1]['Test Accuracy']

print(f"\nBest Model for Predicting '{most_consumed_drug}': {best_model_name} with Accuracy = {best_model_accuracy:.4f}")

drug_user_counts = df[drug_labels].sum().sort_values(ascending=False)
most_consumed_drug = drug_user_counts.idxmax()
print(f"Most Consumed Drug: {most_consumed_drug}")

"""DATA VISVUALIZATION - COMPARING RESULTS"""

splits = [0.1, 0.2, 0.3, 0.5]
split_labels = [int(s * 100) for s in splits]


plot_data = []
drug = most_consumed_drug

# Iterate through models for the most consumed drug
for model_name, metrics in results[drug].items():
    accuracy = metrics['Test Accuracy']
    for split, split_label in zip(splits, split_labels):
        plot_data.append({'Model': model_name, 'Split': split_label, 'Accuracy': accuracy})

plot_df = pd.DataFrame(plot_data)
plt.figure(figsize=(12, 6))
sns.barplot(data=plot_df, x='Split', y='Accuracy', hue='Model', palette='viridis')

plt.title(f"Model Accuracy vs Train-Test Split for {drug}", fontsize=14)
plt.xlabel("Test Size (%)", fontsize=12)
plt.ylabel("Test Accuracy", fontsize=12)
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
plt.grid(True)
plt.tight_layout()
plt.show()

"""STEP-06: FUTURE TREND ANALYSIS USING LSTM AND ARIMA MODEL"""

# Prepare data for predicted (test) accuracy comparison
comparison_data = []

drug = most_consumed_drug

for model_name, metrics in results[drug].items():
    #actual_acc = metrics['Cross-Validated Accuracy'] # This line is removed
    predicted_acc = metrics['Test Accuracy']
    #comparison_data.append({'Model': model_name, 'Type': 'Cross-Validated Accuracy', 'Accuracy': actual_acc}) # This line is removed
    comparison_data.append({'Model': model_name, 'Type': 'Test Accuracy', 'Accuracy': predicted_acc})

# Convert to DataFrame
comparison_df = pd.DataFrame(comparison_data)

# Plot
plt.figure(figsize=(10, 6))
sns.barplot(data=comparison_df, x='Model', y='Accuracy', hue='Type', palette='magma')

plt.title(f"Predicted Accuracy for Models â€“ {drug}", fontsize=14) # Changed title
plt.xlabel("Model", fontsize=12)
plt.ylabel("Accuracy", fontsize=12)
plt.xticks(rotation=45)
plt.grid(True, axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.legend(title="Accuracy Type", loc='upper right')
plt.show()

referenced_accuracies = {
    'Logistic Regression': 0.49,
    'Support Vector Machine': 0.57,
    'Random Forest': 0.92,
    'Decision Tree': 0.87,
    'Naive Bayes': 0.35,
    'K-Nearest Neighbors': 0.97,
    'XGBoost': 0.93,
    'MLP': 0.89,
    'CatBoost': 0.95,
    'Ridge Classifier': 0.78,
    'SGD Classifier': 0.55,
    'AdaBoost Classifier': 0.90,
    'LightGBM': 0.96
}

drug = most_consumed_drug
comparison_data = []


for model_name in results[drug]:
    your_model_accuracy = results[drug][model_name]['Test Accuracy']

    reference_accuracy = referenced_accuracies.get(model_name, None)

    comparison_data.append({'Model': model_name, 'Type': 'Proposed', 'Accuracy': your_model_accuracy})
    if reference_accuracy is not None:
        comparison_data.append({'Model': model_name, 'Type': 'Referenced', 'Accuracy': reference_accuracy})

comparison_df = pd.DataFrame(comparison_data)

# Create a bar plot using seaborn
plt.figure(figsize=(12, 6))
sns.barplot(x='Model', y='Accuracy', hue='Type', data=comparison_df, palette='viridis')

plt.title(f'Model Accuracy Comparision: Proposed vs Reference', fontsize=16)
plt.xlabel('Model', fontsize=14)
plt.ylabel('Accuracy', fontsize=14)
plt.xticks(rotation=45, ha='right')
plt.ylim(0, 1)
plt.legend(title='Accuracy Source', loc='upper right', fontsize=12)
plt.tight_layout()
plt.grid(True, axis='y', linestyle='--', alpha=0.7)
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import recall_score

# Choose the drug
selected_drug = 'Choc'
le = LabelEncoder()
y = le.fit_transform(df[selected_drug])

test_sizes = np.arange(0.1, 0.6, 0.1)
recall_scores_by_model = {model_name: [] for model_name in models.keys()}

for test_size in test_sizes:
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=test_size, random_state=42)

    for name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        try:
            recall = recall_score(y_test, y_pred)
        except:
            recall = 0  # fallback in case of error
        recall_scores_by_model[name].append(recall)

# Plotting
plt.figure(figsize=(12, 8))
for model_name, scores in recall_scores_by_model.items():
    plt.plot(test_sizes, scores, marker='o', linestyle='--', label=model_name)

plt.title(f'Recall vs Test Split Size')
plt.xlabel('Test Split Size')
plt.ylabel('Recall Score')
plt.legend(loc='lower left', bbox_to_anchor=(1, 0))
plt.grid(True)
plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score

# Choose the drug
selected_drug = 'Choc'
le = LabelEncoder()
y = le.fit_transform(df[selected_drug])

test_sizes = np.arange(0.1, 0.6, 0.1)
precision_scores_by_model = {model_name: [] for model_name in models.keys()}

for test_size in test_sizes:
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=test_size, random_state=42)

    for name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        try:
            precision = precision_score(y_test, y_pred)
        except:
            precision = 0  # fallback in case of error
        precision_scores_by_model[name].append(precision)

# Plotting
plt.figure(figsize=(12, 8))
for model_name, scores in precision_scores_by_model.items():
    plt.plot(test_sizes, scores, marker='s', linestyle='--', label=model_name)

plt.title(f'Precision vs Test Split Size')
plt.xlabel('Test Split Size')
plt.ylabel('Precision Score')
plt.legend(loc='lower left', bbox_to_anchor=(1, 0))
plt.grid(True)
plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import f1_score

# Choose the drug
selected_drug = 'Choc'
le = LabelEncoder()
y = le.fit_transform(df[selected_drug])

test_sizes = np.arange(0.1, 0.6, 0.1)
f1_scores_by_model = {model_name: [] for model_name in models.keys()}

for test_size in test_sizes:
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=test_size, random_state=42)

    for name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        try:
            f1 = f1_score(y_test, y_pred)
        except:
            f1 = 0  # fallback in case of error
        f1_scores_by_model[name].append(f1)

# Plotting
plt.figure(figsize=(12, 8))
for model_name, scores in f1_scores_by_model.items():
    plt.plot(test_sizes, scores, marker='^', linestyle='--', label=model_name)

plt.title(f'F1-Score vs Test Split Size')
plt.xlabel('Test Split Size')
plt.ylabel('F1-Score')
plt.legend(loc='lower left', bbox_to_anchor=(1, 0))
plt.grid(True)
plt.tight_layout()
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming your drug columns have binary values (0 for non-user and 1 for user)
drug_labels = ['Alcohol', 'Amphet', 'Caff', 'Amyl', 'Benzos', 'Cannabis', 'Choc', 'Crack',
               'Ecstasy', 'Heroin', 'Ketamine', 'Legalh', 'LSD', 'Meth', 'Mushrooms',
               'Nicotine', 'Semer', 'VSA']

# Calculate usage rates
usage_rates = df[drug_labels].mean().sort_values(ascending=False)

# Print the most consumed drug
most_consumed_drug = usage_rates.index[0]  # Get the drug name with highest usage rate
print(f"Most consumed drug: {most_consumed_drug}")

# Print consumption percentages for each drug
print("\nConsumption percentages for each drug:")
for drug, rate in usage_rates.items():
    percentage = rate * 100
    print(f"{drug}: {percentage:.2f}%")

# Force 'Choc' to be the first in the list for correlation matrix
most_consumed_drugs = usage_rates.index.tolist()  # Get all drugs sorted by usage
most_consumed_drugs.insert(0, most_consumed_drugs.pop(most_consumed_drugs.index('Choc')))  # Move 'Choc' to the beginning

# Compute and plot the correlation matrix
corr_matrix = df[most_consumed_drugs].corr()

plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Matrix of Most Consumed Drugs (with Choc highlighted)")
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

# give plot for AUC-ROC Analysis
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder


drug_to_plot = 'Choc'

# Initialize auc_data dictionary
auc_data = {}
auc_data[drug_to_plot] = {}  # Initialize for the drug

# Get features and target
le = LabelEncoder()
y = le.fit_transform(df[drug_to_plot])  # Assuming 'df' and 'drug_to_plot' are defined

# Split data (you might need to adjust this based on your previous code)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42) # Adjust test_size if needed

# Iterate through models to calculate AUC
for name, model in models.items():
    model.fit(X_train, y_train)

    # Get prediction scores (probabilities for positive class)
    try:
        y_score = model.predict_proba(X_test)[:, 1]
    except AttributeError:
        # Handle models without predict_proba (e.g., SVC with linear kernel)
        y_score = model.decision_function(X_test)

    # Store data for plotting
    auc_data[drug_to_plot][name] = {'y_test': y_test, 'y_score': y_score}

# Now you can plot the AUC-ROC curves
plt.figure(figsize=(10, 8))
for model_name, data in auc_data[drug_to_plot].items():
    fpr, tpr, _ = roc_curve(data['y_test'], data['y_score'])
    auc_score = roc_auc_score(data['y_test'], data['y_score'])
    plt.plot(fpr, tpr, label=f"{model_name} (AUC = {auc_score:.2f})")

plt.plot([0, 1], [0, 1], 'k--', label='Random Chance')
plt.title(f"AUC-ROC Curves for {drug_to_plot}")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.grid(True)
plt.tight_layout()
plt.show()

# give plot for AUC-ROC Analysis
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Assuming 'drug_labels' is defined earlier in your code
drug_labels = ['Alcohol', 'Amphet', 'Caff', 'Amyl', 'Benzos', 'Cannabis', 'Choc', 'Crack',
               'Ecstasy', 'Heroin', 'Ketamine', 'Legalh', 'LSD', 'Meth', 'Mushrooms',
               'Nicotine', 'Semer', 'VSA']

# Initialize auc_data dictionary for all drugs
auc_data = {}

# Iterate through each drug
for drug_to_plot in drug_labels:
    auc_data[drug_to_plot] = {}  # Initialize for the current drug

    # Get features and target
    le = LabelEncoder()
    y = le.fit_transform(df[drug_to_plot])

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

    # Iterate through models to calculate AUC for the current drug
    for name, model in models.items():
        model.fit(X_train, y_train)
        try:
            y_score = model.predict_proba(X_test)[:, 1]
        except AttributeError:
            y_score = model.decision_function(X_test)
        auc_data[drug_to_plot][name] = {'y_test': y_test, 'y_score': y_score}

    # Plot AUC-ROC curves for the current drug
    plt.figure(figsize=(10, 8))
    for model_name, data in auc_data[drug_to_plot].items():
        fpr, tpr, _ = roc_curve(data['y_test'], data['y_score'])
        auc_score = roc_auc_score(data['y_test'], data['y_score'])
        plt.plot(fpr, tpr, label=f"{model_name} (AUC = {auc_score:.2f})")

    plt.plot([0, 1], [0, 1], 'k--', label='Random Chance')
    plt.title(f"AUC-ROC Curves for {drug_to_plot}")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.legend(loc="lower right")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

"""ANALYSIS OF DCR BASED ON FUTURE TRENDS"""

import tensorflow as tf
from tensorflow import keras

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import LSTM, Dense

# Simulate historical data for 'Choc' drug consumption
date_range = pd.date_range(start='2000-01-01', end='2023-12-01', freq='M')
np.random.seed(42)
choc_values = np.random.normal(loc=2.5, scale=0.5, size=len(date_range))
df = pd.DataFrame({'Date': date_range, 'Choc': choc_values})
df.set_index('Date', inplace=True)
df = df.asfreq('M')

# Normalize data
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df[['Choc']])

# Prepare sequences
sequence_length = 12
X, y = [], []
for i in range(sequence_length, len(scaled_data)):
    X.append(scaled_data[i-sequence_length:i])
    y.append(scaled_data[i])
X, y = np.array(X), np.array(y)

# LSTM model
model = Sequential()
model.add(LSTM(64, activation='relu', input_shape=(X.shape[1], 1)))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')
model.fit(X, y, epochs=20, batch_size=16, verbose=0)

# Forecast until 2060
n_months = (2060 - 2023) * 12
last_sequence = scaled_data[-sequence_length:]
forecast_scaled = []

for _ in range(n_months):
    input_seq = last_sequence.reshape(1, sequence_length, 1)
    pred = model.predict(input_seq, verbose=0)
    forecast_scaled.append(pred[0][0])
    last_sequence = np.append(last_sequence[1:], pred).reshape(sequence_length, 1)

forecast = scaler.inverse_transform(np.array(forecast_scaled).reshape(-1, 1))
future_dates = pd.date_range(start=df.index[-1] + pd.DateOffset(months=1), periods=n_months, freq='M')
forecast_series = pd.Series(forecast.flatten(), index=future_dates)

# Plotting
plt.figure(figsize=(14, 6))
plt.plot(df['Choc'], label='Historical')
plt.plot(forecast_series, label='LSTM Forecast until 2060', linestyle='--', color='orange')
plt.title('LSTM Forecast for Choc Drug Consumption (2000â€“2060)')
plt.xlabel('Year')
plt.ylabel('Consumption Level')
plt.legend()
plt.grid(True)
plt.tight_layout()

# Save the plot as an image file
plot_path = '/mnt/data/choc_forecast_lstm.png'
plt.savefig(plot_path)

plot_path

import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA

# Simulated sample data for 'Choc' drug consumption
# (since original dataset is unavailable in this environment)
# Generating monthly data from 2000 to 2023
date_range = pd.date_range(start='2000-01-01', end='2023-12-01', freq='M')
import numpy as np
np.random.seed(42)
choc_values = np.random.normal(loc=2.5, scale=0.5, size=len(date_range))  # simulate realistic consumption

df = pd.DataFrame({'Date': date_range, 'Choc': choc_values})
df.set_index('Date', inplace=True)
df = df.asfreq('M')

# Fit ARIMA
model = ARIMA(df['Choc'], order=(1, 1, 1))
model_fit = model.fit()

# Forecast until 2060
n_months = (2060 - 2023) * 12
forecast = model_fit.forecast(steps=n_months)
future_dates = pd.date_range(start=df.index[-1] + pd.DateOffset(months=1), periods=n_months, freq='M')
forecast_series = pd.Series(forecast, index=future_dates)

# Plot
plt.figure(figsize=(14, 6))
plt.plot(df['Choc'], label='Historical')
plt.plot(forecast_series, label='Forecast until 2060', linestyle='--')
plt.title('ARIMA Forecast for Choc Drug Consumption (2000â€“2060)')
plt.xlabel('Year')
plt.ylabel('Consumption Level')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()